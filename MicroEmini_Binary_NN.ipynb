!pip install -q xgboost
!pip install -q keras
!pip install -q tensorflow
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import numpy as np
import tensorflow as tf
import pandas as pd
import boto3
from sagemaker import get_execution_role
import sagemaker

from sklearn.model_selection import train_test_split
from keras import models
#from keras import layers
from tensorflow import keras
from tensorflow.keras import layers
#from keras import optimizers
from tensorflow.keras import optimizers
from keras.models import Input, Model
from keras.layers import Dense
from keras.layers import Dropout

# Use SageMaker
SageMaker = True

import sys
if sys.version_info[0] < 3: 
    from StringIO import StringIO
else:
    from io import StringIO

# Use back-test files
back_test = False
if back_test:
  bt_frag = '-back'
else:
  bt_frag = ''

# spread predictions

# Set scaled boolean
sclaled_on = True

if not SageMaker:
    from google.colab import drive
    drive.mount('/content/drive')
    #!ls "/content/drive/My Drive/LSTM Futures"
    #!cp '/content/drive/My Drive/LSTM Futures/Micro-Emini-SP500-MES-F-granular' + bt_frag + '.csv' 'Micro-Emini-SP500-MES-F-granular' + bt_frag + '.csv'
    #!cp '/content/drive/My Drive/LSTM Futures/daily-granular' + bt_frag + '.csv' 'daily-granular' + bt_frag + '.csv'
    EminiSP = pd.read_csv('/content/drive/My Drive/LSTM Futures/Micro-Emini-SP500-MES-F-granular' + bt_frag + '.csv')
    EminiSPpredicted = pd.read_csv('/content/drive/My Drive/LSTM Futures/daily-granular' + bt_frag + '.csv')
else:
    sagemaker_session = sagemaker.Session()
    role = get_execution_role()
    bucket='eminisp500vbloise'
    train_data_key = 'Micro-Emini-SP500-MES-F-granular' + bt_frag + '.csv'
    inference_data_key = 'daily-granular' + bt_frag + '.csv'
    train_data_location = 's3://{}/{}'.format(bucket, train_data_key)
    inference_data_location = 's3://{}/{}'.format(bucket, inference_data_key)
    EminiSP = pd.read_csv(train_data_location)
    EminiSPpredicted = pd.read_csv(inference_data_location)

X = EminiSP.copy()
X.drop(['date-1', 'time-1', 'time-2', 'time-3', 'time-4', 'time-5', 'outcome', 'b_outcome'], axis=1, inplace=True) #,'percent-change-1', 'percent-change-2', 'percent-change-3'
y = EminiSP.pop('b_outcome')
EminiSPpredict = EminiSPpredicted.copy()
EminiSPpredict.drop(['date-1', 'time-1', 'time-2', 'time-3', 'time-4', 'time-5'], axis=1, inplace=True) #,'percent-change-1', 'percent-change-2', 'percent-change-3'

print(X.head())
print(X.shape)

print(y.head())
print(y.shape)

print(EminiSPpredict.head())
print(EminiSPpredict.shape)

#
# Set up the network
#
#
# Create training and test split
#
# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
#TESTDATA = StringIO("""open-1;high-1;low-1;close-1;volume-1;open-2;high-2;low-2;close-2;volume-2;open-3;high-3;low-3;close-3;volume-3;open-4;high-4;low-4;close-4;volume-4;open-5;high-5;low-5;close-5;volume-5
#  1;4348;4348;4346.25;4346.75;432;4355.75;4356.25;4353.5;4354.5;1576;4365.25;4366.25;4362.5;4363;2238;4351.5;4354.25;4350;4354;2804;4347;4348.25;4346.5;4347.5;1348
#  """)
#df_test = pd.read_csv(TESTDATA, sep=";")
#EminiSPpredict = df_test
# Scaling the data
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(y)
encoded_y = encoder.transform(y)
encoder.fit(y_train)
encoded_y_train = encoder.transform(y_train)
encoder.fit(y_test)
encoded_y_test = encoder.transform(y_test)
from sklearn.preprocessing import MinMaxScaler

Target_scaler = MinMaxScaler(feature_range=(0, 1))
Feature_scaler = MinMaxScaler(feature_range=(0, 1))

X_train_scaled = Feature_scaler.fit_transform(np.array(X_train))
X_test_scaled = Feature_scaler.fit_transform(np.array(X_test))
X_scaled = Feature_scaler.fit_transform(np.array(X))

y_train_scaled = Target_scaler.fit_transform(np.array(encoded_y_train).reshape(-1,1))
y_test_scaled = Target_scaler.fit_transform(np.array(encoded_y_test).reshape(-1,1))
y_scaled = Target_scaler.fit_transform(np.array(encoded_y).reshape(-1,1))
EminiSPpredict_scaled = Feature_scaler.fit_transform(np.array(EminiSPpredict))

def create_baseline():
  # create model
  n_layers = 5
  inputs = Input(shape=(25,))
  x = Dense(25, activation='relu')(inputs)
  x = Dropout(0.4)(x)
  for layer in range(n_layers - 1):
    x = Dense(25, activation='relu')(x)
    #x = Dropout(0.3)(x)
  output = Dense(1, activation='sigmoid')(x) #sigmoid or softmax
  model = Model(inputs, output)
  # Compile model
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

# evaluate model with standardized dataset
estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)
kfold = StratifiedKFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator, X_scaled, encoded_y, cv=kfold)
# evaluate baseline model with standardized dataset
#estimators = []
#estimators.append(('standardize', StandardScaler()))
#estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0))) #build_fn=create_baseline
#pipeline = Pipeline(estimators)
#kfold = StratifiedKFold(n_splits=10, shuffle=True)
#results = cross_val_score(pipeline, X_train_scaled, encoded_y_train, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))
estimator.fit(X_scaled, encoded_y)
pred = estimator.predict(EminiSPpredict_scaled) #EminiSPpredict_scaled or X_test_scaled
pred_rescaled = Target_scaler.inverse_transform(pred)
print("Prediction: ", pred_rescaled)


