import numpy as np
import tensorflow as tf
import pandas as pd
from google.colab import drive
from sklearn.model_selection import train_test_split
from keras import models
from keras import layers
#from keras import optimizers
from tensorflow.keras import optimizers
from keras.models import Input, Model
from keras.layers import Dense
from keras.layers import Dropout

# spread predictions

# Set scaled boolean
sclaled_on = True

drive.mount('/content/drive')
#!ls "/content/drive/My Drive/LSTM Futures"
!cp "/content/drive/My Drive/LSTM Futures/Micro-Emini-SP500-MES-F-granular.csv" "Micro-Emini-SP500-MES-F-granular.csv"
!cp "/content/drive/My Drive/LSTM Futures/daily-granular.csv" "daily-granular.csv"
EminiSP = pd.read_csv("/content/drive/My Drive/LSTM Futures/Micro-Emini-SP500-MES-F-granular.csv")
EminiSPpredict = pd.read_csv("/content/drive/My Drive/LSTM Futures/daily-granular.csv")

X = EminiSP.copy()
X.drop(['date-1', 'time-1', 'time-2', 'time-3', 'time-4', 'time-5', 'outcome'], axis=1, inplace=True) #,'percent-change-1', 'percent-change-2', 'percent-change-3'
y = EminiSP.pop('outcome')
EminiSPpredict.drop(['date-1', 'time-1', 'time-2', 'time-3', 'time-4', 'time-5'], axis=1, inplace=True) #,'percent-change-1', 'percent-change-2', 'percent-change-3'

print(X.head())
print(X.shape)

print(y.head())
print(y.shape)

#
# Set up the network
#
#network = models.Sequential()
#network.add(layers.Dense(24, activation='relu', input_shape=(12,)))
#network.add(layers.Dense(32, activation='relu'))
#network.add(layers.Dense(32, activation='relu'))
#network.add(layers.Dense(32, activation='relu'))
#network.add(layers.Dense(32, activation='relu'))
#network.add(layers.Dense(1))

# Try Keras Functional API
#inputs = Input(shape=(25,))
#output = Dense(64, activation='linear')(inputs)
#linear_model = Model(inputs, output)
#linear_model.compile(optimizer='sgd', loss='mse')

# specify how many hidden layers to add (min 1)
n_layers = 5

inputs = Input(shape=(25,))
x = Dense(200, activation='relu')(inputs)
#x = Dropout(0.4)(x)
for layer in range(n_layers - 1):
  x = Dense(200, activation='relu')(x)
  #x = Dropout(0.3)(x)
output = Dense(1, activation='linear')(x)
deep_n_net = Model(inputs, output)

#
# Configure the network with optimizer, loss function and accuracy
#
#network.compile(optimizer=optimizers.RMSprop(learning_rate=0.01),
#                loss='mse',
#                metrics=['mae'])
#
# Create training and test split
#
# Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
# Scaling the data
from sklearn.preprocessing import MinMaxScaler

Target_scaler = MinMaxScaler(feature_range=(0, 1))
Feature_scaler = MinMaxScaler(feature_range=(0, 1))

X_train_scaled = Feature_scaler.fit_transform(np.array(X_train))
X_test_scaled = Feature_scaler.fit_transform(np.array(X_test))

y_train_scaled = Target_scaler.fit_transform(np.array(y_train).reshape(-1,1))
y_test_scaled = Target_scaler.fit_transform(np.array(y_test).reshape(-1,1))
EminiSPpredict_scaled = Feature_scaler.fit_transform(np.array(EminiSPpredict))

# functional model
#deep_n_net.compile(optimizer = 'adam', loss= 'mse', metrics=['mae'])
deep_n_net.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name="Adam"), loss= 'mse', metrics=['mae'])

if not sclaled_on:
  history = deep_n_net.fit(X_train, y_train, epochs = 100, verbose=1,
                           validation_data = (X_test, y_test))
else:
  history = deep_n_net.fit(X_train_scaled, y_train_scaled, epochs = 100, verbose=1,
                           validation_data = (X_test_scaled, y_test_scaled))
#deep_n_net.fit(X_train, y_train, epochs = 1, verbose=1,
#  validation_data = (X_test, y_test))


#
# Fit the network
#
#history = network.fit(X_train, y_train,
#                    validation_data=(X_test, y_test),
#                    epochs=18,
#                    batch_size=20)

import matplotlib.pyplot as plt
 
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
accuracy = history_dict['mae']
val_accuracy = history_dict['val_mae']
 
epochs = range(1, len(loss_values) + 1)
fig, ax = plt.subplots(1, 2, figsize=(14, 6))
#
# Plot the model accuracy (MAE) vs Epochs
#
ax[0].plot(epochs, accuracy, 'bo', label='Training accuracy')
ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')
ax[0].set_title('Training & Validation Accuracy', fontsize=16)
ax[0].set_xlabel('Epochs', fontsize=16)
ax[0].set_ylabel('Accuracy', fontsize=16)
ax[0].legend()
#
# Plot the loss vs Epochs
#
ax[1].plot(epochs, loss_values, 'bo', label='Training loss')
ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')
ax[1].set_title('Training & Validation Loss', fontsize=16)
ax[1].set_xlabel('Epochs', fontsize=16)
ax[1].set_ylabel('Loss', fontsize=16)
ax[1].legend()

# make predictions
if not sclaled_on:
  y_pred = deep_n_net.predict(EminiSPpredict)
  print(y_pred)
else:
  y_pred = deep_n_net.predict(EminiSPpredict_scaled)
  print(y_pred)

#y_pred = network.predict(EminiSPpredict)
#y_pred = model.predict(EminiSPpredict)
# the predictions also lie in that range. We need to scale them back in the other direction
#y_pred_rescaled = Target_scaler.inverse_transform(y_pred)
#print(y_pred)
